{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "075c72cf-9205-44c9-9fd8-b72a126261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r /root/FACIAL/video_preprocess/test1.wav /root/FACIAL/examples/audio/test1.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5760c6-9826-46cc-9fe7-0d5af597064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20K\t/root/miniconda3/pkgs/\n",
      "1.7M\t/root/.local/share/Trash\n"
     ]
    }
   ],
   "source": [
    "!du -sh /root/miniconda3/pkgs/ && rm -rf /root/miniconda3/pkgs/*      # conda的历史包\n",
    "!du -sh /root/.local/share/Trash && rm -rf /root/.local/share/Trash   # jupyterlab的回收站"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f4dbf-2623-451e-9296-6e49810e7fef",
   "metadata": {},
   "source": [
    "3.3 Audio preprocess (CPU ~20m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2e192b-6177-44c5-bcd8-cc92c4f6af2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/FACIAL/audio2face\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:101: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:101: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:102: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:102: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:105: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:105: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [03:41<00:00, 221.64s/it]\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:148: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/FACIAL/audio2face/_utils/audio_handler.py:148: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /root/FACIAL/audio2face\n",
    "\n",
    "! python audio_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89baa3da-1dce-43f3-9f92-2c3f7389e6a6",
   "metadata": {},
   "source": [
    "3.2. Test audio2face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22bbb2c2-342c-4f2a-a056-b918e93dbf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/FACIAL/audio2face\n",
      "/root/FACIAL/examples/audio_preprocessed/test1.pkl\n"
     ]
    }
   ],
   "source": [
    "%cd /root/FACIAL/audio2face\n",
    "! python test.py --audiopath /root/FACIAL/examples/audio_preprocessed/test1.pkl --checkpath /root/FACIAL/audio2face/checkpoint/train1/Gen-10.mdl"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6ef304b-64ae-4956-ba1f-4f532168ad3c",
   "metadata": {},
   "source": [
    "4.1. Run 3D face rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e25138c-6abd-41be-91da-3ae04cd022f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/FACIAL/examples/rendering/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c3de9c-efd3-49b1-9742-e6d4ac9b5a9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/FACIAL/face_render\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "%cd /root/FACIAL/face_render/\n",
    "\n",
    "! python rendering_gaosi.py --train_params_path /root/FACIAL/video_preprocess/train1_posenew.npz --net_params_path ../examples/test-result/test1.npz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d38236-3597-4df3-ad36-67e6124b04bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/FACIAL\n"
     ]
    }
   ],
   "source": [
    "%cd /root/FACIAL\n",
    "!rm -rf /root/FACIAL/face2vid/datasets/train3/train_A\n",
    "!rm -rf /root/FACIAL/face2vid/datasets/train3/train_B\n",
    "%cp -r /root/FACIAL/video_preprocess/train_A /root/FACIAL/face2vid/datasets/train3/train_A\n",
    "%cp -r /root/FACIAL/video_preprocess/train1_image /root/FACIAL/face2vid/datasets/train3/train_B\n",
    "\n",
    "%rm -rf /root/FACIAL/face2vid/datasets/train3/test_A /root/FACIAL/face2vid/datasets/train3/test_B\n",
    "%cp -r /root/FACIAL/examples/rendering/test1 /root/FACIAL/face2vid/datasets/train3/test_A\n",
    "%cp -r /root/FACIAL/examples/rendering/test1 /root/FACIAL/face2vid/datasets/train3/test_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44251de6-7573-4325-b422-e60b980e772f",
   "metadata": {},
   "source": [
    "5.2 Test face2video¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d36771f8-bbcd-4445-89c1-8119558ae2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/FACIAL/examples/test_image/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed7dc7a-5f35-4e8f-ad86-557907d1fabd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/FACIAL/face2vid\n",
      "------------ Options -------------\n",
      "aspect_ratio: 1.0\n",
      "batchSize: 1\n",
      "blink_path: ../examples/test-result/test1.npz\n",
      "checkpoints_dir: ./checkpoints\n",
      "clip_length: 500\n",
      "cluster_path: features_clustered_010.npy\n",
      "data_type: 32\n",
      "dataroot: ./datasets/train3/\n",
      "display_winsize: 512\n",
      "engine: None\n",
      "export_onnx: None\n",
      "feat_num: 3\n",
      "fineSize: 512\n",
      "gpu_ids: [0]\n",
      "input_nc: 3\n",
      "instance_feat: False\n",
      "isTrain: False\n",
      "label_feat: False\n",
      "label_nc: 0\n",
      "loadSize: 512\n",
      "load_features: False\n",
      "max_dataset_size: inf\n",
      "model: pose2vid\n",
      "nThreads: 4\n",
      "n_blocks_global: 4\n",
      "n_blocks_local: 3\n",
      "n_clusters: 10\n",
      "n_downsample_E: 4\n",
      "n_downsample_global: 4\n",
      "n_local_enhancers: 1\n",
      "name: train3\n",
      "nef: 16\n",
      "netG: local\n",
      "ngf: 32\n",
      "niter_fix_global: 0\n",
      "no_flip: False\n",
      "no_instance: True\n",
      "norm: instance\n",
      "ntest: inf\n",
      "onnx: None\n",
      "output_nc: 3\n",
      "phase: test\n",
      "resize_or_crop: resize\n",
      "results_dir: ./results/\n",
      "serial_batches: False\n",
      "test_id_name: test1\n",
      "tf_log: False\n",
      "use_dropout: False\n",
      "use_first_frame: False\n",
      "verbose: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "CustomDatasetDataLoader\n",
      "dataset [AlignedPairDataset] was created\n",
      "(65,)\n",
      "65 10\n",
      "#test images = 65\n",
      "LocalEnhancer(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(30, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (21): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (24): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (27): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (30): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (31): ReLU(inplace=True)\n",
      "  )\n",
      "  (model1_1): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(30, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "  )\n",
      "  (model1_2): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (7): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (8): Tanh()\n",
      "  )\n",
      "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
      ")\n",
      "0\n",
      "/root/FACIAL/face2vid/models/pose2vidHD_model.py:154: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  input_label = Variable(input_label, volatile=infer)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "< cv2.VideoWriter 0x7f1ef0268410>\n"
     ]
    }
   ],
   "source": [
    "%cd /root/FACIAL/face2vid\n",
    "! python test_video.py --test_id_name test1 --blink_path '../examples/test-result/test1.npz' --name train3 --model pose2vid --dataroot ./datasets/train3/ --which_epoch latest --netG local --ngf 32 --label_nc 0 --n_local_enhancers 1 --no_instance --resize_or_crop resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "733fd40a-5cd9-48fc-8073-007ca595fe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, avi, from '../examples/test_image/test1/test_1.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:02.17, start: 0.000000, bitrate: 4098 kb/s\n",
      "    Stream #0:0: Video: mjpeg (Baseline) (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 512x512, 4135 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : mono\n",
      "\u001b[0mInput #1, wav, from '/root/FACIAL/video_preprocess/test1.wav':\n",
      "  Duration: 00:00:02.17, bitrate: 352 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 22050 Hz, mono, s16, 352 kb/s\n",
      "Output #0, avi, to '../examples/test_image/test1/test_1_audio.avi':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0: Video: mjpeg (Baseline) (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 512x512, q=2-31, 4135 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
      "    Stream #0:1: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 22050 Hz, mono, s16, 352 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "frame=   65 fps=0.0 q=-1.0 Lsize=    1182kB time=00:00:02.16 bitrate=4470.2kbits/s speed=1.8e+03x    \n",
      "video:1077kB audio:94kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.008510%\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.1 : mono\n",
      "\u001b[0mInput #0, avi, from '../examples/test_image/test1/test_1_audio.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:02.17, start: 0.000000, bitrate: 4470 kb/s\n",
      "    Stream #0:0: Video: mjpeg (Baseline) (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 512x512, 4135 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
      "    Stream #0:1: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 22050 Hz, mono, s16, 352 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mprofile High, level 3.0\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '../examples/test_image/test1/test_1_audio.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc, progressive), 512x512, q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 22050 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "frame=   65 fps=0.0 q=-1.0 Lsize=      46kB time=00:00:02.18 bitrate= 172.0kbits/s speed=  16x    \n",
      "video:24kB audio:19kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 7.239954%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mframe I:1     Avg QP:17.05  size: 15391\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mframe P:16    Avg QP:19.28  size:   440\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mframe B:48    Avg QP:32.05  size:    32\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mconsecutive B-frames:  1.5%  0.0%  0.0% 98.5%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mmb I  I16..4:  1.3% 98.6%  0.1%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mmb P  I16..4:  0.0%  0.4%  0.0%  P16..4:  5.9%  0.4%  0.5%  0.0%  0.0%    skip:92.7%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.8%  0.0%  0.0%  direct: 0.0%  skip:98.2%  L0:33.0% L1:67.0% BI: 0.0%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0m8x8 transform intra:98.4% inter:78.9%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mcoded y,uvDC,uvAC intra: 80.9% 69.8% 20.6% inter: 0.7% 0.7% 0.1%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mi16 v,h,dc,p: 36%  0% 36% 29%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33% 22% 26%  3%  2%  4%  3%  4%  4%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 33% 17% 11%  0%  2%  3%  6%  0%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mi8c dc,h,v,p: 37% 21% 34%  8%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mref P L0: 90.7%  0.3%  3.7%  5.3%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mref B L0: 64.8% 29.3%  5.8%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mref B L1: 92.4%  7.6%\n",
      "\u001b[1;36m[libx264 @ 0x562aaf2a13c0] \u001b[0mkb/s:88.56\n",
      "\u001b[1;36m[aac @ 0x562aaf2a2980] \u001b[0mQavg: 134.643\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "audio_path ='/root/FACIAL/video_preprocess/test1.wav'\n",
    "video_new='../examples/test_image/test1/test_1.avi'\n",
    "output = '../examples/test_image/test1/test_1_audio.avi'\n",
    "output_mp4 = '../examples/test_image/test1/test_1_audio.mp4'\n",
    "!ffmpeg -i '../examples/test_image/test1/test_1.avi' -i '/root/FACIAL/video_preprocess/test1.wav' -c copy '../examples/test_image/test1/test_1_audio.avi'\n",
    "!ffmpeg -i '../examples/test_image/test1/test_1_audio.avi'  '../examples/test_image/test1/test_1_audio.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927fd8c-1315-44f9-91d4-b6fdd21c4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/autodl-fs/train3_checkpoints.zip /root/FACIAL/face2vid/checkpoints/train3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da92b1-c2a4-4520-902e-ffaa806caba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(output_mp4, \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(f\"\"\"\n",
    "<video width=\"20%\" height=\"20%\" controls>\n",
    "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
